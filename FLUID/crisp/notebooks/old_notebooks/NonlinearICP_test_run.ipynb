{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1467fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/linus/projects/fdl/crisp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ebf5742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/linus/projects/fdl/crisp/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad4f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47935cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Deconfounder import *\n",
    "from models.NonLinearInvariantCausalPrediction import NonLinearInvariantCausalPrediction\n",
    "from dataio.DataFrameDataset import *\n",
    "from dataio.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6cb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic.facebook_synthetic_data_generator import generator_example\n",
    "\n",
    "n_example = 2\n",
    "dim_inv=2\n",
    "n_bin=1\n",
    "dim_spu=2\n",
    "n_exp=2000 #int(2e2)\n",
    "n_env=2\n",
    "save_dir= 'data/synthetic'\n",
    "test=False \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b4562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"Example Experiment for AH casual ensemble\",\n",
    "    \"short_name\": \"ah_experiment_notebook\",\n",
    "    \"bucket_project\": \"fdl-us-astronaut-health\",\n",
    "    \"bucket_name\": \"ah_21_data\",\n",
    "    \"bucket_path\": \"gs://ah_21_data\",\n",
    "    \"verbose\": 1,\n",
    "    \"test_val_split\": [0.1, 0.1],\n",
    "    \"per_variant_experiment\": False,\n",
    "    \"data_options\": {\n",
    "        #'dataset_fp': '../data/test_multiclass.pkl',\n",
    "        'dataset_fp' : None,\n",
    "        'output_data_regime' : [\"real-valued\", \"binary\", \"binary\", \"None\", \"multi-class\", \"real-valued\"],\n",
    "        'subject_keys': 'Subj_ID',\n",
    "        'targets': ['Target'],\n",
    " #        'predictors': ['All'],\n",
    "        'predictors': None,\n",
    "        'environments': ['env_split'],\n",
    "        'exclude': ['Subj_ID']\n",
    "    },\n",
    "    \"feature_selection_options\": {\n",
    "        \"max_features\": 20,\n",
    "        \"verbose\": 0,\n",
    "        \"seed\": 12\n",
    "    },\n",
    "    \"ensemble_options\": {\n",
    "        \"models\": [\"ERM\", \"RF\", \"ICP\", \"IRM\", \"DCF\", \"ITE\", \"LIRM\", \"NLICP\"]\n",
    "    },\n",
    "    \"use_cloud\":False,\n",
    "    \"results_directory\": \"results/\"\n",
    "}\n",
    "data_config = config['data_options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39852117",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLICP_options = config.get(\"ensemble_options\").get('NLICP', {})\n",
    "NLICP_args = {\n",
    "            \"max_set_size\": 2,\n",
    "            \"alpha\": 0.05,\n",
    "            \"seed\": 12,\n",
    "            \"verbose\": 1,\n",
    "            \"method\": \"MLP\",\n",
    "            \"hidden_dim\": 256\n",
    "        }\n",
    "NLICP_args.update(NLICP_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b61de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "n_example: 1 \n",
      "\n",
      "\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 1\n",
      "Running a per sample experiment\n",
      "Loaded  2  train environments\n",
      "Env  0  has  808  samples\n",
      "X shape  (808, 4)  y shape  (808, 1)\n",
      "Env  1  has  812  samples\n",
      "X shape  (812, 4)  y shape  (812, 1)\n",
      "Loaded val set, X shape: (180, 4)  y shape:  (180, 1)\n",
      "Loaded test set, X shape: (200, 4)  y shape:  (200, 1)\n",
      "no accepted sets found for nonlinear ICP\n",
      "\n",
      "\n",
      "n_example: 2 \n",
      "\n",
      "\n",
      "Environments variables: {'E0': {'p': 0.95, 's': 0.3}, 'E1': {'p': 0.97, 's': 0.5}}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 2\n",
      "Running a per sample experiment\n",
      "Loaded  2  train environments\n",
      "Env  0  has  826  samples\n",
      "X shape  (826, 4)  y shape  (826, 1)\n",
      "Env  1  has  794  samples\n",
      "X shape  (794, 4)  y shape  (794, 1)\n",
      "Loaded val set, X shape: (180, 4)  y shape:  (180, 1)\n",
      "Loaded test set, X shape: (200, 4)  y shape:  (200, 1)\n",
      "no accepted sets found for nonlinear ICP\n",
      "\n",
      "\n",
      "n_example: 3 \n",
      "\n",
      "\n",
      "Environments variables: {'E0': tensor([[ 1.5410, -0.2934]]), 'E1': tensor([[-2.1788,  0.5684]])}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 3\n",
      "Running a per sample experiment\n",
      "Loaded  2  train environments\n",
      "Env  0  has  816  samples\n",
      "X shape  (816, 4)  y shape  (816, 1)\n",
      "Env  1  has  804  samples\n",
      "X shape  (804, 4)  y shape  (804, 1)\n",
      "Loaded val set, X shape: (180, 4)  y shape:  (180, 1)\n",
      "Loaded test set, X shape: (200, 4)  y shape:  (200, 1)\n",
      "Accepted subset: (0,)\n",
      "Accepted subset: (1,)\n",
      "Accepted subset: (0, 1)\n",
      "Intersection: []\n",
      "Pruning defining set trees\n",
      "Defining Sets: [[0, 1]]\n",
      "\n",
      "\n",
      "n_example: 5 \n",
      "\n",
      "\n",
      "Environments variables: {'E0': {'p': [0.8, 0.1, 0.1], 's': [0.3, 0.4, 0.3]}, 'E1': {'p': [0.2, 0.1, 0.7], 's': [0.5, 0.4, 0.1]}}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 5\n",
      "Running a per sample experiment\n",
      "Loaded  2  train environments\n",
      "Env  0  has  786  samples\n",
      "X shape  (786, 4)  y shape  (786, 1)\n",
      "Env  1  has  834  samples\n",
      "X shape  (834, 4)  y shape  (834, 1)\n",
      "Loaded val set, X shape: (180, 4)  y shape:  (180, 1)\n",
      "Loaded test set, X shape: (200, 4)  y shape:  (200, 1)\n",
      "no accepted sets found for nonlinear ICP\n",
      "\n",
      "\n",
      "n_example: 6 \n",
      "\n",
      "\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 6\n",
      "Running a per sample experiment\n",
      "Loaded  2  train environments\n",
      "Env  0  has  808  samples\n",
      "X shape  (808, 4)  y shape  (808, 1)\n",
      "Env  1  has  812  samples\n",
      "X shape  (812, 4)  y shape  (812, 1)\n",
      "Loaded val set, X shape: (180, 4)  y shape:  (180, 1)\n",
      "Loaded test set, X shape: (200, 4)  y shape:  (200, 1)\n",
      "no accepted sets found for nonlinear ICP\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for n_example in [1,2,3,5,6]:\n",
    "    print(\"\\n\\nn_example:\", n_example, \"\\n\\n\")\n",
    "    generator_example(n_example, dim_inv, dim_spu, n_exp, n_env ,save_dir, test, n_bin)\n",
    "    outfile = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp, n_env,test))\n",
    "    if n_example == 6:\n",
    "        outfile = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s_n_bin_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp, n_env,test,n_bin))\n",
    "    df_train = pd.read_pickle(outfile)\n",
    "    df_train = df_train[df_train[\"env_split\"]==0]\n",
    "    \n",
    "    #df_train = df_train.drop(columns=[\"env_split\", \"Subj_ID\"])\n",
    "    #df_train.head()\n",
    "    config[\"data_options\"][\"dataset_fp\"] = outfile\n",
    "    config[\"data_options\"][\"predictors\"] = list(df_train.columns[0:dim_inv+dim_spu])\n",
    "    environment_datasets, val_dataset, test_dataset = get_datasets_for_experiment(config)\n",
    "\n",
    "    NLICP_args[\"target\"] = config[\"data_options\"][\"targets\"]\n",
    "    NLICP_args[\"output_data_regime\"] = config[\"data_options\"][\"output_data_regime\"][n_example-1]\n",
    "    NLICP_args[\"columns\"] = test_dataset.predictor_columns\n",
    "\n",
    "    nlicp = NonLinearInvariantCausalPrediction(environment_datasets, val_dataset, test_dataset, NLICP_args)\n",
    "    if not hasattr(nlicp, 'test_logits'): # nlicp.test() was not run after training bc no sets accepted -> nlicp.results() won't work\n",
    "        nlicp_results_dict = {\"result\" : \"no-accepted-sets\"}\n",
    "    else:\n",
    "        nlicp_results_dict = nlicp.results()\n",
    "    \n",
    "    results[n_example] = nlicp_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92255130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from synthetic.synthetic_generator import synthetic_generator\n",
    "#df = synthetic_generator()\n",
    "#df[\"Target\"] = np.random.randint(0,3,size=df.shape[0])\n",
    "#df.to_pickle(\"../data/test_multiclass.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "#with open(\"../data/test.pkl\",'rb') as file:\n",
    "with open(\"../data/test_multiclass.pkl\",'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_datasets, val_dataset, test_dataset = get_datasets_for_experiment(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ad20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DCF\n",
    "dcf_args = {\n",
    "    \"minP\": 0.1,\n",
    "    \"maxP\": 0.9,\n",
    "    \"minFeatures\": 1,\n",
    "    \"minAccuracy\": 0.5,\n",
    "    \"seed\": 0,\n",
    "    \"verbose\": 1,\n",
    "    \"target\": data_config['targets'],\n",
    "    \"output_pvals\": False # we cannot output pvalues in torch on the current build \n",
    "}\n",
    "dcf_args[\"columns\"] = test_dataset.predictor_columns\n",
    "\n",
    "dcf = TorchMultiClassDeconfounder(environment_datasets, val_dataset, test_dataset, dcf_args)\n",
    "\n",
    "dcf_results_dict = dcf.predictor_results()\n",
    "print(\"Finished DCF\")\n",
    "print(dcf_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcf_results_dict[\"results\"][\"Features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcf_results_dict[\"results\"][\"Features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DCF\n",
    "dcf_args = {\n",
    "    \"minP\": 0.1,\n",
    "    \"maxP\": 0.9,\n",
    "    \"minFeatures\": 1,\n",
    "    \"minAccuracy\": 0.5,\n",
    "    \"seed\": 0,\n",
    "    \"verbose\": 1,\n",
    "    \"target\": data_config['targets'],\n",
    "    \"output_pvals\": False # we cannot output pvalues in torch on the current build \n",
    "}\n",
    "dcf_args[\"columns\"] = test_dataset.predictor_columns\n",
    "\n",
    "dcf = Deconfounder(environment_datasets, val_dataset, test_dataset, dcf_args)\n",
    "\n",
    "dcf_results_dict = dcf.predictor_results()\n",
    "print(\"Finished DCF\")\n",
    "print(dcf_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_ICP_results_dict['selected_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910d64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m74"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
