{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(1, '/home/linus/projects/fdl/crisp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/linus/projects/fdl/crisp'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(\"..\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LinearInvariantRiskMinimization import LinearInvariantRiskMinimization\n",
    "from dataio.DataFrameDataset import *\n",
    "from dataio.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic.facebook_synthetic_data_generator import generator_example\n",
    "\n",
    "dim_inv = 2\n",
    "dim_spu = 2\n",
    "n_exp_train = 6 #int(2e2)\n",
    "n_exp_test = 6\n",
    "n_env = 3\n",
    "save_dir = 'data/synthetic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"Example Experiment for AH casual ensemble\",\n",
    "    \"short_name\": \"ah_experiment_notebook\",\n",
    "    \"bucket_project\": \"fdl-us-astronaut-health\",\n",
    "    \"bucket_name\": \"ah_21_data\",\n",
    "    \"bucket_path\": \"gs://ah_21_data\",\n",
    "    \"verbose\": 1,\n",
    "    \"test_val_split\": [0.1, 0.1],\n",
    "    \"per_variant_experiment\": False,\n",
    "    \"data_options\": {\n",
    "        #'dataset_fp': '../data/test_multiclass.pkl',\n",
    "        'dataset_fp' : None,\n",
    "        'output_data_regime' : [\"real-valued\", \"binary\", \"binary\", \"real-valued\", \"multi-class\"],\n",
    "        'subject_keys': 'Subj_ID',\n",
    "        'targets': ['Target'],\n",
    " #        'predictors': ['All'],\n",
    "        'predictors': None,\n",
    "        'environments': ['env_split'],\n",
    "        'exclude': ['Subj_ID'],\n",
    "        'synthetic_train_test_split' : True,\n",
    "    },\n",
    "    \"feature_selection_options\": {\n",
    "        \"max_features\": 20,\n",
    "        \"verbose\": 0,\n",
    "        \"seed\": 12\n",
    "    },\n",
    "    \"ensemble_options\": {\n",
    "        \"models\": [\"ERM\", \"RF\", \"ICP\", \"IRM\", \"DCF\", \"ITE\", \"LIRM\", \"NLICP\"]\n",
    "    },\n",
    "    \"use_cloud\":False,\n",
    "    \"results_directory\": \"results/\"\n",
    "}\n",
    "data_config = config['data_options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIRM_options = config.get(\"ensemble_options\").get('LIRM', {})\n",
    "LIRM_args = {\n",
    "            \"use_icp_initialization\": False,\n",
    "            \"verbose\": 1,\n",
    "            \"n_iterations\": 3000, # 1000\n",
    "            \"seed\": 0,\n",
    "            \"lr\": 0.005, # 0.001\n",
    "            \"cuda\": False\n",
    "        }\n",
    "LIRM_args.update(LIRM_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_synthetic_experiment(n_example, dim_inv, dim_spu, n_exp_train, n_exp_test, n_env, save_dir):\n",
    "    # training data\n",
    "    generator_example(n_example, dim_inv, dim_spu, n_exp_train, n_env ,save_dir, False)\n",
    "    outfile_train = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp_train, n_env, False))\n",
    "\n",
    "    # test data\n",
    "    generator_example(n_example, dim_inv, dim_spu, n_exp_test, n_env ,save_dir, True)\n",
    "    outfile_test = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp_test, n_env, True))\n",
    "    \n",
    "    return outfile_train, outfile_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Causal_0</th>\n",
       "      <th>Causal_1</th>\n",
       "      <th>Non_causal_0</th>\n",
       "      <th>Non_causal_1</th>\n",
       "      <th>env_split</th>\n",
       "      <th>Subj_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006890</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>-0.872451</td>\n",
       "      <td>-0.734993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003430</td>\n",
       "      <td>-0.004423</td>\n",
       "      <td>-0.227449</td>\n",
       "      <td>-0.127548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.012709</td>\n",
       "      <td>-0.006520</td>\n",
       "      <td>-1.179085</td>\n",
       "      <td>-0.882011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.717926</td>\n",
       "      <td>0.522778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012972</td>\n",
       "      <td>0.012450</td>\n",
       "      <td>0.591929</td>\n",
       "      <td>0.749831</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>-0.006603</td>\n",
       "      <td>-0.227203</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Causal_0  Causal_1  Non_causal_0  Non_causal_1  env_split  Subj_ID  Target\n",
       "0 -0.006890  0.001798     -0.872451     -0.734993        0.0      0.0     2.0\n",
       "1 -0.003430 -0.004423     -0.227449     -0.127548        0.0      1.0     2.0\n",
       "2 -0.012709 -0.006520     -1.179085     -0.882011        1.0      2.0     2.0\n",
       "3  0.006613  0.010388      0.717926      0.522778        1.0      3.0     1.0\n",
       "4  0.012972  0.012450      0.591929      0.749831        2.0      4.0     1.0\n",
       "5  0.000607  0.003996     -0.006603     -0.227203        2.0      5.0     0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "n_example: 5 output_data_regime: multi-class \n",
      "\n",
      "\n",
      "Environments variables: {'E0': {'p': [0.8, 0.1, 0.1], 's': [0.3, 0.4, 0.3]}, 'E1': {'p': [0.9, 0.05, 0.05], 's': [0.4, 0.3, 0.3]}, 'E2': {'p': [0.97, 0.01, 0.02], 's': [0.3, 0.3, 0.4]}}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 5\n",
      "Environments variables: {'E0': {'p': [0.8, 0.1, 0.1], 's': [0.3, 0.4, 0.3]}, 'E1': {'p': [0.9, 0.05, 0.05], 's': [0.4, 0.3, 0.3]}, 'E2': {'p': [0.97, 0.01, 0.02], 's': [0.3, 0.3, 0.4]}}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 5\n",
      "Running a per sample experiment\n",
      "Using synthetic dataset train/test split\n",
      "Loaded  3  train environments\n",
      "Env  0  has  1  samples\n",
      "X shape  (1, 14)  y shape  (1, 1)\n",
      "Env  1  has  1  samples\n",
      "X shape  (1, 14)  y shape  (1, 1)\n",
      "Env  2  has  1  samples\n",
      "X shape  (1, 14)  y shape  (1, 1)\n",
      "Loaded test set, X shape: (2, 14)  y shape:  (2, 1)\n",
      "Start IRM training procedure\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0w/dyx4srhd6sj4xq1p54wvjx_c0000gn/T/ipykernel_1607/1450965675.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mLIRM_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mlirm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearInvariantRiskMinimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLIRM_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlirm_results_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlirm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/fdl/crisp/models/LinearInvariantRiskMinimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, environment_datasets, val_dataset, test_dataset, args)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/fdl/crisp/models/LinearInvariantRiskMinimization.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0merror_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                     penalty += torch.autograd.grad(error_e, self.w,\n\u001b[1;32m    134\u001b[0m                                                    create_graph=True)[0].pow(2).mean()\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1121\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for n_example in [5]:#,2,3,4,5]:\n",
    "    output_data_regime = config[\"data_options\"][\"output_data_regime\"][n_example-1]\n",
    "    print(\"\\n\\nn_example:\", n_example, \"output_data_regime:\", output_data_regime, \"\\n\\n\")\n",
    "    \n",
    "    outfile_train, outfile_test = get_dataset_for_synthetic_experiment(n_example, dim_inv, dim_spu, n_exp_train, n_exp_test, n_env, save_dir)\n",
    "    df_train = pd.read_pickle(outfile_train)\n",
    "    df_test = pd.read_pickle(outfile_test)\n",
    "\n",
    "    config[\"data_options\"][\"dataset_fp_train\"] = outfile_train\n",
    "    config[\"data_options\"][\"dataset_fp_test\"] = outfile_test\n",
    "    config[\"data_options\"][\"predictors\"] = list(df_train.columns[0:dim_inv+dim_spu])\n",
    "    environment_datasets, val_dataset, test_dataset = get_datasets_for_experiment(config)\n",
    "\n",
    "    LIRM_args[\"target\"] = config[\"data_options\"][\"targets\"]\n",
    "    LIRM_args[\"output_data_regime\"] = output_data_regime\n",
    "    LIRM_args[\"columns\"] = test_dataset.predictor_columns\n",
    "\n",
    "    lirm = LinearInvariantRiskMinimization(environment_datasets, val_dataset, test_dataset, LIRM_args)\n",
    "    \n",
    "    lirm_results_dict = lirm.results()\n",
    "    results[n_example] = lirm_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test_logits', 'test_acc', 'test_nll', 'test_probs', 'test_labels', 'feature_coeffients', 'to_bucket'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('acc/r2:', tensor(0.2576))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results.get(n_example).keys())\n",
    "\"acc/r2:\", results.get(n_example).get(\"test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Causal_2', 28.2161705493927),\n",
       " ('Causal_1', 26.71197199821472),\n",
       " ('Causal_0', 26.41683316230774),\n",
       " ('Causal_3', 21.73872461915016),\n",
       " ('Non_causal_7', 2.7639010846614838),\n",
       " ('Non_causal_5', 2.7266255617141724),\n",
       " ('Non_causal_2', 2.3941598534584045),\n",
       " ('Non_causal_8', 2.0637218952178955),\n",
       " ('Non_causal_6', 1.5529405996203423),\n",
       " ('Non_causal_4', 1.2595409452915192),\n",
       " ('Non_causal_3', 1.1885599195957184),\n",
       " ('Non_causal_9', 0.9855049755424261),\n",
       " ('Non_causal_1', 0.6325856596231461),\n",
       " ('Non_causal_0', 0.30464520677924156)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = results.get(n_example).get(\"to_bucket\")\n",
    "\n",
    "if output_data_regime == \"multi-class\":\n",
    "    r = sorted(zip(result.get(\"features\"), np.abs(result.get(\"coefficients\")).sum(axis=1)), key=lambda x: x[1], reverse=True)\n",
    "else:\n",
    "    r = sorted(zip(result.get(\"features\"), result.get(\"coefficients\")), key=lambda x: abs(x[1]), reverse=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m74"
  },
  "kernelspec": {
   "display_name": "crisp-env-kernel",
   "language": "python",
   "name": "crisp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

