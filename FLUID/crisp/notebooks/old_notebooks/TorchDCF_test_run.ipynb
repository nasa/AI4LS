{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdae1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(1, '/home/linus/projects/fdl/crisp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666fa280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/crisp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(\"..\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7167a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Deconfounder import *\n",
    "from models.TorchMultiClassDeconfounder import *\n",
    "from dataio.DataFrameDataset import *\n",
    "from dataio.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ed89cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic.facebook_synthetic_data_generator import generator_example\n",
    "\n",
    "#n_example = 1\n",
    "dim_inv=5\n",
    "n_bin=0\n",
    "dim_spu=10\n",
    "n_exp_train = 1000\n",
    "n_exp_test = 400\n",
    "n_env=1\n",
    "save_dir= 'data/synthetic'\n",
    "test=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6726dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"Example Experiment for AH casual ensemble\",\n",
    "    \"short_name\": \"ah_experiment_notebook\",\n",
    "    \"bucket_project\": \"fdl-us-astronaut-health\",\n",
    "    \"bucket_name\": \"ah_21_data\",\n",
    "    \"bucket_path\": \"gs://ah_21_data\",\n",
    "    \"verbose\": 1,\n",
    "    \"test_val_split\": [0.1, 0.1],\n",
    "    \"per_variant_experiment\": False,\n",
    "    \"data_options\": {\n",
    "        #'dataset_fp': '../data/test_multiclass.pkl',\n",
    "        'dataset_fp' : None,\n",
    "        'output_data_regime' : [\"real-valued\", \"binary\", \"binary\", \"None\", \"multi-class\", \"real-valued\"],\n",
    "        'subject_keys': 'Subj_ID',\n",
    "        'targets': ['Target'],\n",
    " #        'predictors': ['All'],\n",
    "        'predictors': None,\n",
    "        'environments': ['env_split'],\n",
    "        'exclude': ['Subj_ID'],\n",
    "        'synthetic_train_test_split' : True # use this to split in synthetic_generator\n",
    "    },\n",
    "    \"feature_selection_options\": {\n",
    "        \"max_features\": 20,\n",
    "        \"verbose\": 0,\n",
    "        \"seed\": 12\n",
    "    },\n",
    "    \"ensemble_options\": {\n",
    "        \"models\": [\"ERM\", \"RF\", \"ICP\", \"IRM\", \"DCF\", \"ITE\", \"LIRM\", \"NLICP\"]\n",
    "    },\n",
    "    \"use_cloud\" : False,\n",
    "    \"results_directory\": \"results/\"\n",
    "}\n",
    "data_config = config['data_options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae63eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcf_args = {\n",
    "    \"minP\": 0.1,\n",
    "    \"maxP\": 0.9,\n",
    "    \"minFeatures\": 1,\n",
    "    \"minAccuracy\": 0.5,\n",
    "    \"seed\": 0,\n",
    "    \"verbose\": 1,\n",
    "    \"target\": data_config['targets'],\n",
    "    \"output_pvals\": False # we cannot output pvalues in torch on the current build \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a792328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "n_example: 2 output_data_regime: binary \n",
      "\n",
      "\n",
      "Environments variables: {'E0': {'p': 0.95, 's': 0.3}}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 2\n",
      "Environments variables: {'E0': {'p': 0.95, 's': 0.3}}\n",
      "Generated Synthetic Data according to the Facebook setup Example: 2\n",
      "Running a per sample experiment\n",
      "Loaded  1  train environments\n",
      "Env  0  has  800  samples\n",
      "X shape  (800, 15)  y shape  (800, 1)\n",
      "Loaded val set, X shape: (200, 15)  y shape:  (200, 1)\n",
      "Loaded test set, X shape: (206, 15)  y shape:  (206, 1)\n",
      "testshape: (206, 16)\n",
      "valshape: (200, 16)\n",
      "trainshape: (800, 16)\n",
      "Number of features: 15\n",
      "to drop: []\n",
      "dropped 0 std columns of self.dfX\n",
      "Number of features after correlated and 0 std features elimination: 6\n",
      "Inferred W:\n",
      "[[0.7105555  0.71261823 0.7523504  0.757472   0.73638386 0.7467199 ]]\n",
      "Standard Deviation:\n",
      "[[0.00432342 0.00618804 0.00530751 0.00560048 0.00559799 0.00530775]]\n",
      "pval: 0.13906921241050121\n",
      "predictions: (200, 1)\n",
      "truth: (200,)\n",
      "Acc/r_squared: 100.0\n",
      "dcf_coefs: [[0.37838104 0.        ]\n",
      " [0.72326785 0.        ]\n",
      " [0.17218882 0.        ]\n",
      " [0.1842519  0.        ]\n",
      " [0.34790894 0.        ]\n",
      " [0.20818448 0.        ]\n",
      " [0.33263725 0.        ]]\n",
      "modelout features:                  value  pval\n",
      "Causal_3      0.378381   0.0\n",
      "Causal_4      0.723268   0.0\n",
      "Non_causal_2  0.172189   0.0\n",
      "Non_causal_5  0.184252   0.0\n",
      "Non_causal_7  0.347909   0.0\n",
      "Non_causal_9  0.208184   0.0\n",
      "Z1            0.332637   0.0\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for n_example in [2]: #[1,2,3,5]:\n",
    "    config[\"data_options\"][\"synthetic_train_test_split\"] = True\n",
    "    output_data_regime = config[\"data_options\"][\"output_data_regime\"][n_example-1]\n",
    "    print(\"\\n\\nn_example:\", n_example, \"output_data_regime:\", output_data_regime, \"\\n\\n\")\n",
    "    generator_example(n_example, dim_inv, dim_spu, n_exp_train, n_env ,save_dir, test=False)#, n_bin) # generate pickle with train data\n",
    "    generator_example(n_example, dim_inv, dim_spu, n_exp_test, n_env ,save_dir, test=True)  # generate pickle with test data\n",
    "    outfile_train = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp_train, n_env,\"False\"))\n",
    "    outfile_test = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp_test, n_env,\"True\"))\n",
    "    \n",
    "    if n_example == 6:\n",
    "        outfile_train = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s_n_bin_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp, n_env,\"False\",n_bin))\n",
    "        outfile_test = os.path.join(save_dir,\"data_fb_example_%s_dim_inv_%s_dim_spu_%s_n_exp_%s_n_env_%s_test_%s_n_bin_%s.pickle\" %(n_example, dim_inv, dim_spu, n_exp, n_env,\"True\",n_bin))\n",
    "        \n",
    "    df_train = pd.read_pickle(outfile_train)\n",
    "    df_test = pd.read_pickle(outfile_test)\n",
    "\n",
    "    config[\"data_options\"][\"dataset_fp_train\"] = outfile_train\n",
    "    config[\"data_options\"][\"dataset_fp_test\"] = outfile_test\n",
    "    config[\"data_options\"][\"predictors\"] = list(df_train.columns[0:dim_inv+dim_spu])\n",
    "    environment_datasets, val_dataset, test_dataset = get_datasets_for_experiment(config)\n",
    "\n",
    "    dcf_args[\"target\"] = config[\"data_options\"][\"targets\"]\n",
    "    dcf_args[\"output_data_regime\"] = output_data_regime\n",
    "    dcf_args[\"columns\"] = test_dataset.predictor_columns\n",
    "\n",
    "    dcf = TorchMultiClassDeconfounder(environment_datasets, val_dataset, test_dataset, dcf_args)\n",
    "    dcf_results_dict = dcf.predictor_results()\n",
    "    \n",
    "    results[n_example] = dcf_results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddde3e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['method', 'features', 'coefficients', 'pvals', 'test_acc', 'test_acc_std'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = results.get(n_example).get(\"to_bucket\")\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5487f8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'Deconfounder',\n",
       " 'features': None,\n",
       " 'coefficients': None,\n",
       " 'pvals': None,\n",
       " 'test_acc': 0.0,\n",
       " 'test_acc_std': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "344d1422",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-05e5b14f11b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coefficients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coefficients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "if output_data_regime == \"multi-class\":\n",
    "    r = sorted(zip(result.get(\"features\"), np.abs(result.get(\"coefficients\")).sum(axis=1)), key=lambda x: x[1], reverse=True)\n",
    "else:\n",
    "    r = sorted(zip(result.get(\"features\"), result.get(\"coefficients\")), key=lambda x: abs(x[1]), reverse=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8caf58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from synthetic.synthetic_generator import synthetic_generator\n",
    "#df = synthetic_generator()\n",
    "#df[\"Target\"] = np.random.randint(0,3,size=df.shape[0])\n",
    "#df.to_pickle(\"../data/test_multiclass.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eee02480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "#with open(\"../data/test.pkl\",'rb') as file:\n",
    "with open(\"../data/test_multiclass.pkl\",'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3116210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a per sample experiment\n",
      "Loaded  2  train environments\n",
      "Env  0  has  406  samples\n",
      "X shape  (406, 4)  y shape  (406, 1)\n",
      "Env  1  has  404  samples\n",
      "X shape  (404, 4)  y shape  (404, 1)\n",
      "Loaded val set, X shape: (90, 4)  y shape:  (90, 1)\n",
      "Loaded test set, X shape: (100, 4)  y shape:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "environment_datasets, val_dataset, test_dataset = get_datasets_for_experiment(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f63e8205",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testshape: (100, 5)\n",
      "valshape: (90, 5)\n",
      "trainshape: (810, 5)\n",
      "Number of features: 4\n",
      "to drop: []\n",
      "dropped 0 std columns of self.dfX\n",
      "Number of features after correlated and 0 std features elimination: 2\n",
      "Inferred W:\n",
      "[[0.84723186 0.8350464 ]]\n",
      "Standard Deviation:\n",
      "[[0.00468905 0.00620451]]\n",
      "pval: 0.2171940298507463\n",
      "IF TRIGGERED (1, 3)\n",
      "predictions: [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "truth: [0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Acc: 45.0\n",
      "dcf_coefs: [[ 0.63526577  0.        ]\n",
      " [-0.27188298  0.        ]\n",
      " [-0.02029716  0.        ]]\n",
      "modelout features:                  value  pval\n",
      "Causal_1      0.635266   0.0\n",
      "Non_causal_1 -0.271883   0.0\n",
      "Z1           -0.020297   0.0\n",
      "Finished DCF\n",
      "{'solution': True, 'latent_representation': True, 'results': {'test_accuracy': 0.92, 'test_accuracy_std': 0.2712931993250107, 'Features':                  value  pval\n",
      "Causal_1      0.635266   0.0\n",
      "Non_causal_1 -0.271883   0.0\n",
      "Z1           -0.020297   0.0, 'train_accuracy': 45.0, 'confusion_matrix': [[42, 8], [0, 50]], 'numFeatures': 3}, 'test_acc': 0.92, 'to_bucket': {'method': 'Deconfounder', 'features': ['Causal_1', 'Non_causal_1', 'Z1'], 'coefficients': [0.6352657675743103, -0.2718829810619354, -0.020297156646847725], 'pvals': [0.0, 0.0, 0.0], 'test_acc': 0.92, 'test_acc_std': 0.2712931993250107}}\n"
     ]
    }
   ],
   "source": [
    "# run DCF\n",
    "dcf_args = {\n",
    "    \"minP\": 0.1,\n",
    "    \"maxP\": 0.9,\n",
    "    \"minFeatures\": 1,\n",
    "    \"minAccuracy\": 0.5,\n",
    "    \"seed\": 0,\n",
    "    \"verbose\": 1,\n",
    "    \"target\": data_config['targets'],\n",
    "    \"output_pvals\": False # we cannot output pvalues in torch on the current build \n",
    "}\n",
    "dcf_args[\"columns\"] = test_dataset.predictor_columns\n",
    "\n",
    "dcf = TorchMultiClassDeconfounder(environment_datasets, val_dataset, test_dataset, dcf_args)\n",
    "\n",
    "dcf_results_dict = dcf.predictor_results()\n",
    "print(\"Finished DCF\")\n",
    "print(dcf_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77c8a7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Causal_1</th>\n",
       "      <td>0.635266</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_1</th>\n",
       "      <td>-0.271883</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z1</th>\n",
       "      <td>-0.020297</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 value  pval\n",
       "Causal_1      0.635266   0.0\n",
       "Non_causal_1 -0.271883   0.0\n",
       "Z1           -0.020297   0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcf_results_dict[\"results\"][\"Features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14fe411e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value_1</th>\n",
       "      <th>value_2</th>\n",
       "      <th>pval</th>\n",
       "      <th>pval_1</th>\n",
       "      <th>pval_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non_causal_0</th>\n",
       "      <td>0.278975</td>\n",
       "      <td>-0.199761</td>\n",
       "      <td>-0.217077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_1</th>\n",
       "      <td>0.018610</td>\n",
       "      <td>-0.189191</td>\n",
       "      <td>-0.017036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_2</th>\n",
       "      <td>0.064372</td>\n",
       "      <td>0.103798</td>\n",
       "      <td>0.132453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_3</th>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.279146</td>\n",
       "      <td>0.219328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_4</th>\n",
       "      <td>-0.205513</td>\n",
       "      <td>-0.113096</td>\n",
       "      <td>-0.229808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_5</th>\n",
       "      <td>0.168901</td>\n",
       "      <td>-0.194200</td>\n",
       "      <td>0.055705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Causal_0</th>\n",
       "      <td>-0.130071</td>\n",
       "      <td>0.290094</td>\n",
       "      <td>0.183545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_6</th>\n",
       "      <td>0.007791</td>\n",
       "      <td>-0.213810</td>\n",
       "      <td>0.196901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Causal_1</th>\n",
       "      <td>0.186078</td>\n",
       "      <td>0.030202</td>\n",
       "      <td>-0.106112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non_causal_7</th>\n",
       "      <td>-0.032950</td>\n",
       "      <td>0.175299</td>\n",
       "      <td>0.102825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z1</th>\n",
       "      <td>-0.274719</td>\n",
       "      <td>-0.162222</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 value   value_1   value_2  pval  pval_1  pval_2\n",
       "Non_causal_0  0.278975 -0.199761 -0.217077   0.0     0.0     0.0\n",
       "Non_causal_1  0.018610 -0.189191 -0.017036   0.0     0.0     0.0\n",
       "Non_causal_2  0.064372  0.103798  0.132453   0.0     0.0     0.0\n",
       "Non_causal_3  0.005396  0.279146  0.219328   0.0     0.0     0.0\n",
       "Non_causal_4 -0.205513 -0.113096 -0.229808   0.0     0.0     0.0\n",
       "Non_causal_5  0.168901 -0.194200  0.055705   0.0     0.0     0.0\n",
       "Causal_0     -0.130071  0.290094  0.183545   0.0     0.0     0.0\n",
       "Non_causal_6  0.007791 -0.213810  0.196901   0.0     0.0     0.0\n",
       "Causal_1      0.186078  0.030202 -0.106112   0.0     0.0     0.0\n",
       "Non_causal_7 -0.032950  0.175299  0.102825   0.0     0.0     0.0\n",
       "Z1           -0.274719 -0.162222 -0.163073   0.0     0.0     0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcf_results_dict[\"results\"][\"Features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e310025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testshape: (17, 11)\n",
      "valshape: (11, 11)\n",
      "trainshape: (72, 11)\n",
      "Number of features: 10\n",
      "to drop: []\n",
      "dropped 0 std columns of self.dfX\n",
      "Number of features after correlated and 0 std features elimination: 10\n",
      "WARNING:tensorflow:From /home/jupyter/crisp/models/Deconfounder.py:267: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/crisp/models/Deconfounder.py:203: UserWarning: tfp.edward2 module is deprecated and will be removed on 2019-12-01. Use https://github.com/google/edward2 library instead.\n",
      "  log_joint = ed.make_log_joint_fn(self.ppca_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/crisp/models/Deconfounder.py:270: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/crisp/models/Deconfounder.py:276: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Inferred W:\n",
      "[[-0.10981557  0.1039127  -0.16744502  0.1487232   0.53235054 -0.28796935\n",
      "   0.82654893  0.8187947  -0.98794115  0.6546716 ]]\n",
      "Standard Deviation:\n",
      "[[0.01360007 0.01331744 0.01420823 0.01452198 0.01411408 0.01400418\n",
      "  0.01554985 0.01330545 0.01366075 0.01458096]]\n",
      "pval: 0.01808988764044944\n",
      "Inferred W:\n",
      "[[-0.19818623  0.7873972   0.00285349  0.29324788  0.3645276   0.6014306\n",
      "   0.5623061  -0.09629191 -0.7295748   0.8619132 ]]\n",
      "Standard Deviation:\n",
      "[[0.01415863 0.01535793 0.01560735 0.0138423  0.01492829 0.01394129\n",
      "  0.01444643 0.01243034 0.01364147 0.0154509 ]]\n",
      "pval: 0.03651685393258427\n",
      "Inferred W:\n",
      "[[-0.41313574  0.60376585 -0.39951774  0.16148852  0.14315881  0.6452538\n",
      "   0.42083964 -0.23102091  0.01341048  0.33146822]]\n",
      "Standard Deviation:\n",
      "[[0.06144624 0.06346545 0.06012443 0.06185041 0.06525916 0.04017721\n",
      "  0.05721071 0.05603929 0.06690963 0.05793456]]\n",
      "pval: 0.10333333333333333\n",
      "predictions: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "truth: [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Acc: 0.6363636363636364\n",
      "dcf_coefs: []\n",
      "modelout features: Empty DataFrame\n",
      "Columns: [value, pval]\n",
      "Index: []\n",
      "predictions: [0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "truth: [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Acc: 0.7272727272727273\n",
      "dcf_coefs: [[0.03432066 0.        ]]\n",
      "modelout features:              value  pval\n",
      "Causal_0  0.034321   0.0\n",
      "Finished DCF\n",
      "{'solution': True, 'latent_representation': True, 'results': {'test_accuracy': 0.7058823529411765, 'test_accuracy_std': 0.45564509955381377, 'Features':              value  pval\n",
      "Causal_0  0.034321   0.0, 'train_accuracy': 0.7272727272727273, 'confusion_matrix': [[2, 3], [2, 10]], 'numFeatures': 1}, 'test_acc': 0.7058823529411765, 'to_bucket': {'method': 'Deconfounder', 'features': ['Causal_0'], 'coefficients': [0.03432066398401118], 'pvals': [0.0], 'test_acc': 0.7058823529411765, 'test_acc_std': 0.45564509955381377}}\n"
     ]
    }
   ],
   "source": [
    "# run DCF\n",
    "dcf_args = {\n",
    "    \"minP\": 0.1,\n",
    "    \"maxP\": 0.9,\n",
    "    \"minFeatures\": 1,\n",
    "    \"minAccuracy\": 0.5,\n",
    "    \"seed\": 0,\n",
    "    \"verbose\": 1,\n",
    "    \"target\": data_config['targets'],\n",
    "    \"output_pvals\": False # we cannot output pvalues in torch on the current build \n",
    "}\n",
    "dcf_args[\"columns\"] = test_dataset.predictor_columns\n",
    "\n",
    "dcf = Deconfounder(environment_datasets, val_dataset, test_dataset, dcf_args)\n",
    "\n",
    "dcf_results_dict = dcf.predictor_results()\n",
    "print(\"Finished DCF\")\n",
    "print(dcf_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b018e706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Non_causal_0', 'Non_causal_1', 'Non_causal_2', 'Non_causal_3',\n",
       "       'Non_causal_4', 'Non_causal_5', 'Non_causal_6', 'Causal_0',\n",
       "       'Non_causal_7', 'Non_causal_8'], dtype='<U12')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_ICP_results_dict['selected_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cf90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
