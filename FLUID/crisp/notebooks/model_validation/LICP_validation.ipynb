{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/crisp'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from utils.get_synthetic_data_for_validation import get_datasets\n",
    "from utils.vm_helpers import save_dict_to_json\n",
    "from utils.gcp_helpers import save_json_to_bucket\n",
    "from models.TorchLinearInvariantCausalPrediction import TorchInvariantCausalPrediction\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "icp_args = {\n",
    "    \"minP\": 0.1,\n",
    "    \"maxP\": 0.9,\n",
    "    \"minFeatures\": 1,\n",
    "    \"minAccuracy\": 0.5,\n",
    "    \"alpha\" : 0.01,\n",
    "    \"seed\": 0,\n",
    "    \"verbose\": 0,\n",
    "    \"max_set_size\" : 1,\n",
    "    \"output_pvals\": False,\n",
    "    \"cuda\" : False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example1  seed  0\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5, 'E2': 2, 'E3': 0.30815539086172433, 'E4': 2.016811860476012}\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "Generated Synthetic Data according to the Facebook setup Example: 1\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5, 'E2': 2, 'E3': 0.30815539086172433, 'E4': 2.016811860476012}\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "Generated Synthetic Data according to the Facebook setup Example: 1\n",
      "Running a per sample experiment\n",
      "Using synthetic dataset train/test split\n",
      "Loaded  5  train environments\n",
      "Env  0  has  77  samples\n",
      "X shape  (77, 1020)  y shape  (77, 1)\n",
      "Env  1  has  83  samples\n",
      "X shape  (83, 1020)  y shape  (83, 1)\n",
      "Env  2  has  78  samples\n",
      "X shape  (78, 1020)  y shape  (78, 1)\n",
      "Env  3  has  82  samples\n",
      "X shape  (82, 1020)  y shape  (82, 1)\n",
      "Env  4  has  80  samples\n",
      "X shape  (80, 1020)  y shape  (80, 1)\n",
      "Loaded test set, X shape: (500, 1020)  y shape:  (500, 1)\n",
      "Testing  1021  permutations with max set size:  1\n",
      "1020\n",
      "No intersection, trying defining sets\n",
      "Pruning defining set trees\n",
      "Found the defining sets!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([400])) that is different to the input size (torch.Size([400, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example1  seed  1\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5, 'E2': 2, 'E3': 1.8745403782379346, 'E4': 0.06885454086365675}\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "Generated Synthetic Data according to the Facebook setup Example: 1\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5, 'E2': 2, 'E3': 1.8745403782379346, 'E4': 0.06885454086365675}\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "Generated Synthetic Data according to the Facebook setup Example: 1\n",
      "Running a per sample experiment\n",
      "Using synthetic dataset train/test split\n",
      "Loaded  5  train environments\n",
      "Env  0  has  77  samples\n",
      "X shape  (77, 1020)  y shape  (77, 1)\n",
      "Env  1  has  83  samples\n",
      "X shape  (83, 1020)  y shape  (83, 1)\n",
      "Env  2  has  78  samples\n",
      "X shape  (78, 1020)  y shape  (78, 1)\n",
      "Env  3  has  82  samples\n",
      "X shape  (82, 1020)  y shape  (82, 1)\n",
      "Env  4  has  80  samples\n",
      "X shape  (80, 1020)  y shape  (80, 1)\n",
      "Loaded test set, X shape: (500, 1020)  y shape:  (500, 1)\n",
      "Testing  1021  permutations with max set size:  1\n",
      "1020\n",
      "No intersection, trying defining sets\n",
      "Pruning defining set trees\n",
      "Found the defining sets!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([400])) that is different to the input size (torch.Size([400, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example1  seed  2\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5, 'E2': 2, 'E3': 0.6983694234905468, 'E4': 0.13900760821904815}\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "Generated Synthetic Data according to the Facebook setup Example: 1\n",
      "Environments variables: {'E0': 0.1, 'E1': 1.5, 'E2': 2, 'E3': 0.6983694234905468, 'E4': 0.13900760821904815}\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "torch.Size([100, 1020])\n",
      "Generated Synthetic Data according to the Facebook setup Example: 1\n",
      "Running a per sample experiment\n",
      "Using synthetic dataset train/test split\n",
      "Loaded  5  train environments\n",
      "Env  0  has  77  samples\n",
      "X shape  (77, 1020)  y shape  (77, 1)\n",
      "Env  1  has  83  samples\n",
      "X shape  (83, 1020)  y shape  (83, 1)\n",
      "Env  2  has  78  samples\n",
      "X shape  (78, 1020)  y shape  (78, 1)\n",
      "Env  3  has  82  samples\n",
      "X shape  (82, 1020)  y shape  (82, 1)\n",
      "Env  4  has  80  samples\n",
      "X shape  (80, 1020)  y shape  (80, 1)\n",
      "Loaded test set, X shape: (500, 1020)  y shape:  (500, 1)\n",
      "Testing  1021  permutations with max set size:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-107024010bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0micp_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_options\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predictors\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlicp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchInvariantCausalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0micp_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_regression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlicp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/crisp/models/TorchLinearInvariantCausalPrediction.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_environments, val_environment, test_environment, args, epochs_regression)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchLinearRegressionModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_regression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_environments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/crisp/models/TorchModelZoo.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#clear out the gradients from the last step loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_enter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "examples = [\"Example1\"]#\"Example2\", \"Example3\", \"Example4\", \"Example_Confounder\", \"Example_Nonlinear\"]\n",
    "n_seeds = 10\n",
    "for example in examples:            \n",
    "    results = {}\n",
    "            \n",
    "    for seed in range(n_seeds):\n",
    "        print(example, ' seed ', int(seed))\n",
    "        environment_datasets, val_dataset, test_dataset, config =  get_datasets(example, seed=seed)\n",
    "        icp_args[\"target\"] = config[\"data_options\"][\"targets\"]\n",
    "        icp_args[\"output_data_regime\"] = config[\"data_options\"][\"output_data_regime\"]\n",
    "        icp_args[\"columns\"] = config[\"data_options\"][\"predictors\"]\n",
    "\n",
    "        licp = TorchInvariantCausalPrediction(environment_datasets, val_dataset, test_dataset, icp_args, epochs_regression=500)\n",
    "        results[seed] = licp.results()\n",
    "        results[seed][\"seed\"] = seed\n",
    "            \n",
    "    result_file = \"results/validation/ICP_\" + example + \".json\"\n",
    "\n",
    "    for seed in range(n_seeds):\n",
    "        if results.get(seed)[\"to_bucket\"][\"test_acc\"] is not None:\n",
    "            results.get(seed).getb(\"to_bucket\")[\"test_acc\"] = results.get(seed).get(\"to_bucket\")[\"test_acc\"].item()\n",
    "            results.get(seed)[\"test_acc\"] = results.get(seed)[\"test_acc\"].item()\n",
    "\n",
    "    save_dict_to_json(results, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(10):\n",
    "    results.get(seed)[\"feature_coefficients\"] = results.get(seed)['feature_coeffients'].tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get(0).get('solution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.randint(100)\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Example3\"\n",
    "trained_results = []\n",
    "for seed in range(5):\n",
    "    environment_datasets, val_dataset, test_dataset, config =  get_datasets(example, seed=seed)\n",
    "    licp = TorchInvariantCausalPrediction(environment_datasets, val_dataset, test_dataset, icp_args, epochs_regression=500)\n",
    "    trained_results.append(licp.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = licp.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if example == \"Example5\":\n",
    "    r = sorted(zip(result[\"to_bucket\"][\"features\"], np.abs(result[\"to_bucket\"][\"coefficients\"]).T.sum(axis=1)), key=lambda x: x[1], reverse=True)\n",
    "else:\n",
    "    r = sorted(zip(result[\"to_bucket\"][\"features\"], np.array(result[\"to_bucket\"][\"coefficients\"]).squeeze()), key=lambda x: abs(x[1]), reverse=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results/validation/LICP_\" + example + \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to disc\n",
    "save_dict_to_json(result.get(\"to_bucket\", {\"method\" : \"Linear ICP\"}), result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to gcp bucket\n",
    "save_json_to_bucket(result.get(\"to_bucket\", {\"method\" : \"Linear ICP\"}), result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get(\"to_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp to save some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_all_five_env = [(0.9901584192257143, 'Causal_0_Target'),\n",
    " (0.8462283688046218, 'Causal_5_Target'),\n",
    " (0.7139894327236213, 'Causal_4_Target'),\n",
    " (0.6479747635978667, 'Non_causal_41_Target'),\n",
    " (0.6458195899479231, 'Non_causal_58_Target'),\n",
    " (0.5665979128385252, 'Non_causal_7_Target'),\n",
    " (0.5637456998963807, 'Non_causal_24_Target'),\n",
    " (0.5100658969415852, 'Non_causal_13_Target'),\n",
    " (0.4889323713381053, 'Non_causal_0_Target'),\n",
    " (0.4728059013057373, 'Non_causal_35_Target'),\n",
    " (0.44342284101588625, 'Causal_3_Target'),\n",
    " (0.4228157843217627, 'Non_causal_9_Target'),\n",
    " (0.4168190838881371, 'Non_causal_11_Target'),\n",
    " (0.40954932404960726, 'Non_causal_48_Target'),\n",
    " (0.4084168207947114, 'Non_causal_22_Target'),\n",
    " (0.4045433040714377, 'Non_causal_18_Target'),\n",
    " (0.402401442255577, 'Non_causal_33_Target'),\n",
    " (0.3667189230604628, 'Non_causal_15_Target'),\n",
    " (0.3579495854090835, 'Causal_1_Target'),\n",
    " (0.3573468465200692, 'Non_causal_32_Target'),\n",
    " (0.33305004240128866, 'Non_causal_31_Target'),\n",
    " (0.3166657886949894, 'Non_causal_57_Target'),\n",
    " (0.3151506539299408, 'Non_causal_39_Target'),\n",
    " (0.3032508878717938, 'Non_causal_21_Target'),\n",
    " (0.29480555758046506, 'Non_causal_59_Target'),\n",
    " (0.28546736821936075, 'Non_causal_40_Target'),\n",
    " (0.26927364557626193, 'Non_causal_53_Target'),\n",
    " (0.268817426964354, 'Non_causal_54_Target'),\n",
    " (0.2640268491903321, 'Non_causal_43_Target'),\n",
    " (0.22927934330016278, 'Non_causal_25_Target'),\n",
    " (0.22136424330429622, 'Non_causal_6_Target'),\n",
    " (0.2178367289651581, 'Causal_2_Target'),\n",
    " (0.20581488946315868, 'Non_causal_46_Target'),\n",
    " (0.20492159537218954, 'Non_causal_1_Target'),\n",
    " (0.2036188597170843, 'Non_causal_29_Target'),\n",
    " (0.2006940606105587, 'Non_causal_30_Target'),\n",
    " (0.19974361713208202, 'Non_causal_44_Target'),\n",
    " (0.18697241346188576, 'Non_causal_45_Target'),\n",
    " (0.18330721093323696, 'Non_causal_49_Target'),\n",
    " (0.1573217342898328, 'Non_causal_52_Target'),\n",
    " (0.15722827957528338, 'Non_causal_26_Target'),\n",
    " (0.15428725383479208, 'Non_causal_20_Target'),\n",
    " (0.13900162035416205, 'Non_causal_12_Target'),\n",
    " (0.13836455589827323, 'Non_causal_14_Target'),\n",
    " (0.1296692172070226, 'Non_causal_38_Target'),\n",
    " (0.12203269799539132, 'Non_causal_55_Target'),\n",
    " (0.11940571291779287, 'Non_causal_8_Target'),\n",
    " (0.09914725080020116, 'Non_causal_10_Target'),\n",
    " (0.09847168983271305, 'Non_causal_56_Target'),\n",
    " (0.07941198434673383, 'Non_causal_3_Target'),\n",
    " (0.07775699199604304, 'Non_causal_47_Target'),\n",
    " (0.07769826307687663, 'Non_causal_2_Target'),\n",
    " (0.07135032036688926, 'Non_causal_4_Target'),\n",
    " (0.07024429724771852, 'Non_causal_42_Target'),\n",
    " (0.06479056247237501, 'Non_causal_28_Target'),\n",
    " (0.059497883448267466, 'Non_causal_17_Target'),\n",
    " (0.05449674219087619, 'Non_causal_27_Target'),\n",
    " (0.048613031706336275, 'Non_causal_19_Target'),\n",
    " (0.034594939382216396, 'Non_causal_50_Target'),\n",
    " (0.03217207671607815, 'Non_causal_51_Target'),\n",
    " (0.025429206856269034, 'Non_causal_23_Target'),\n",
    " (0.01841111562613893, 'Non_causal_5_Target'),\n",
    " (0.011344668667387843, 'Non_causal_37_Target'),\n",
    " (0.010769041396014568, 'Non_causal_34_Target'),\n",
    " (0.008091290274722726, 'Non_causal_16_Target'),\n",
    " (0.0038876363776450987, 'Non_causal_36_Target')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(on_all_five_env,open('results/synthetic/CausalNexAllEnv_Example1.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('results/synthetic/CausalNexAllEnv_Example1.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
